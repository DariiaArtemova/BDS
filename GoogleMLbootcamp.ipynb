{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is meant to run in Google Colab."
      ],
      "metadata": {
        "id": "CKZ-uSFeBYTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "DemTlac_B2pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install datasets -q\n",
        "!pip install transformers[torch] -q\n",
        "!pip install kornia -q"
      ],
      "metadata": {
        "id": "QxhGcrE07CyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the dataset from Kaggle"
      ],
      "metadata": {
        "id": "MOeNhbj8BrRK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yas4Khz1trW"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c ukraine-ml-bootcamp-2023\n",
        "!unzip -q ./ukraine-ml-bootcamp-2023.zip  -d ./dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logging into Huggingface"
      ],
      "metadata": {
        "id": "GkrvYAK1Cvqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "OFeYeQQHr_Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating a huggingface dataset"
      ],
      "metadata": {
        "id": "zyIbLG0brnF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "csv = pd.read_csv('/content/dataset/train.csv',\n",
        "                  header=0, names=['file_name', 'label'])\n",
        "csv.to_csv('/content/dataset/images/train_images/metadata.csv', index=False)"
      ],
      "metadata": {
        "id": "tJgdrWOr7gdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"imagefolder\",\n",
        "                       data_dir=\"/content/dataset/images/train_images\",\n",
        "                       split=\"train\")\n",
        "dataset[0], len(dataset)"
      ],
      "metadata": {
        "id": "jhzmBF2p7rap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Saving the dataset to hub and loading it"
      ],
      "metadata": {
        "id": "hO9RKfpsrryw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.push_to_hub(\"dariia-artemova/yoga\", private=True)"
      ],
      "metadata": {
        "id": "WuLg-ypPsH0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"dariia-artemova/yoga\")"
      ],
      "metadata": {
        "id": "V0wlDaFisIuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "-deaqBK7Ac8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#import numpy as np\n",
        "#from torch import nn\n",
        "#from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import v2, InterpolationMode\n",
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "from kornia.augmentation import RandomCutMixV2\n",
        "\n",
        "cutmix = RandomCutMixV2(cut_size=(0.7, 0.85), data_keys=['input', 'class'], p=1)\n",
        "\n",
        "def train_collator(examples):\n",
        "  pixel_values = torch.stack([example['pixel_values'] for example in examples])\n",
        "  labels = torch.tensor([example['label'] for example in examples])\n",
        "  pixel_values, new_labels = cutmix(pixel_values, labels)\n",
        "  new_labels = new_labels.squeeze()\n",
        "  labels = torch.zeros([len(new_labels), 6], dtype=torch.float32)\n",
        "  for i in range(len(new_labels)):\n",
        "    new_label = new_labels[i]\n",
        "    if (new_label[0] == new_label[1]):\n",
        "      labels[i, int(new_label[0])] = 1\n",
        "      continue\n",
        "    labels[i, int(new_label[0])] = 1 - new_label[2]\n",
        "    labels[i, int(new_label[1])] = new_label[2]\n",
        "  return {'pixel_values' : pixel_values, 'labels' : labels}\n",
        "\n",
        "train_transforms = v2.Compose([\n",
        "    v2.RandomResizedCrop(size=(384, 384),\n",
        "                         scale=(0.4, 1),\n",
        "                         ratio=(0.9, 1.1),\n",
        "                         antialias=True,\n",
        "                         interpolation=InterpolationMode.BICUBIC),\n",
        "    v2.ColorJitter(brightness=0.3,\n",
        "                   contrast=0.3,\n",
        "                   saturation=(0.6, 1.2),\n",
        "                   hue=0.02),\n",
        "    v2.RandomGrayscale(p=0.05),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToImageTensor(),\n",
        "    v2.ConvertImageDtype(dtype=torch.float32)\n",
        "    ])\n",
        "\n",
        "def transform_train(examples):\n",
        "  examples['pixel_values'] = [\n",
        "      train_transforms(image.convert('RGB')) for image in examples['image']\n",
        "      ]\n",
        "  return examples\n",
        "\n",
        "dataset.set_transform(transform_train)\n",
        "steps_per_epoch = int(len(dataset)/16)"
      ],
      "metadata": {
        "id": "Toj_HSaZstaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1"
      ],
      "metadata": {
        "id": "cDDN_aj2uDAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = AutoModelForImageClassification.from_pretrained(\n",
        "    'microsoft/cvt-21-384-22k',\n",
        "    num_labels=6,\n",
        "    ignore_mismatched_sizes=True\n",
        "    )"
      ],
      "metadata": {
        "id": "Tlw_yYeauURj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='yoga-competition-cvt-1',\n",
        "    num_train_epochs=100,\n",
        "    remove_unused_columns=False,\n",
        "    per_device_train_batch_size=16,\n",
        "    logging_strategy='epoch',\n",
        "    dataloader_num_workers=2,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to='tensorboard',\n",
        "    weight_decay=0.05,\n",
        "    learning_rate=0.00002,\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_steps=steps_per_epoch*5,\n",
        "    save_strategy='steps',\n",
        "    save_steps=steps_per_epoch*5,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=True,\n",
        "    hub_strategy='all_checkpoints',\n",
        "    hub_private_repo=True\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model1,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=train_collator\n",
        "    )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "VkamAyCN9HSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2"
      ],
      "metadata": {
        "id": "UjXvTVIAuGcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = AutoModelForImageClassification.from_pretrained(\n",
        "    'microsoft/cvt-21-384-22k',\n",
        "    num_labels=6,\n",
        "    ignore_mismatched_sizes=True\n",
        "    )"
      ],
      "metadata": {
        "id": "ynJGGP3guXv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are pretty much the same as in model 1\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='yoga-competition-cvt-2',\n",
        "    num_train_epochs=100,\n",
        "    remove_unused_columns=False,\n",
        "    per_device_train_batch_size=16,\n",
        "    logging_strategy='epoch',\n",
        "    dataloader_num_workers=2,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to='tensorboard',\n",
        "    weight_decay=0.05,\n",
        "    learning_rate=0.00002,\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_steps=steps_per_epoch*5,\n",
        "    save_strategy='steps',\n",
        "    save_steps=steps_per_epoch*5,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=True,\n",
        "    hub_strategy='all_checkpoints',\n",
        "    hub_private_repo=True\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model2,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=train_collator\n",
        "    )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "2GVJxOV7AX6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3"
      ],
      "metadata": {
        "id": "5a7lOdKpuIaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = AutoModelForImageClassification.from_pretrained(\n",
        "    'microsoft/cvt-21-384-22k',\n",
        "    num_labels=6,\n",
        "    ignore_mismatched_sizes=True\n",
        "    )"
      ],
      "metadata": {
        "id": "mwop3uCsBDOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same thing again, just more epochs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='yoga-competition-cvt-3',\n",
        "    num_train_epochs=180,\n",
        "    remove_unused_columns=False,\n",
        "    per_device_train_batch_size=16,\n",
        "    logging_strategy='epoch',\n",
        "    dataloader_num_workers=2,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to='tensorboard',\n",
        "    weight_decay=0.05,\n",
        "    learning_rate=0.00002,\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_steps=steps_per_epoch*5,\n",
        "    save_strategy='steps',\n",
        "    save_steps=steps_per_epoch*5,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=True,\n",
        "    hub_strategy='all_checkpoints',\n",
        "    hub_private_repo=True\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model3,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=train_collator\n",
        "    )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-lSNtLPmBGdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4 (only used in 1 of the 2 selected submissions)"
      ],
      "metadata": {
        "id": "MJAmuu7vMbhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = AutoModelForImageClassification.from_pretrained(\n",
        "    'microsoft/cvt-21-384-22k',\n",
        "    num_labels=6,\n",
        "    ignore_mismatched_sizes=True\n",
        "    )"
      ],
      "metadata": {
        "id": "rQlIFFkGMdxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same thing again, just more epochs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='yoga-competition-cvt-3',\n",
        "    num_train_epochs=100,\n",
        "    remove_unused_columns=False,\n",
        "    per_device_train_batch_size=16,\n",
        "    logging_strategy='epoch',\n",
        "    dataloader_num_workers=2,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to='tensorboard',\n",
        "    weight_decay=0,\n",
        "    learning_rate=0.00002,\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_steps=steps_per_epoch*5,\n",
        "    save_strategy='steps',\n",
        "    save_steps=steps_per_epoch*5,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=True,\n",
        "    hub_strategy='all_checkpoints',\n",
        "    hub_private_repo=True\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model3,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=train_collator\n",
        "    )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "H32jhrDzMi33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Loading your trained models from the hub"
      ],
      "metadata": {
        "id": "VgmLcbPVDKRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "model1 = AutoModelForImageClassification.from_pretrained('dariia-artemova/yoga-competition-cvt-1')\n",
        "model2 = AutoModelForImageClassification.from_pretrained('dariia-artemova/yoga-competition-cvt-2')\n",
        "model3 = AutoModelForImageClassification.from_pretrained('dariia-artemova/yoga-competition-cvt-3')\n",
        "model4 = AutoModelForImageClassification.from_pretrained('dariia-artemova/yoga-competition-cvt-4')"
      ],
      "metadata": {
        "id": "MUU7j3kQDT15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating submisssion.csv"
      ],
      "metadata": {
        "id": "sgsbM9jOAf6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "iHntjnj7aase"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.to(device)\n",
        "model2.to(device)\n",
        "model3.to(device)\n",
        "model4.to(device)\n",
        "model1.eval()\n",
        "model2.eval()\n",
        "model3.eval()\n",
        "model4.eval()"
      ],
      "metadata": {
        "id": "PYHSB7SnEM49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = '/content/dataset/images/test_images/'"
      ],
      "metadata": {
        "id": "1VjClynBE-zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "\n",
        "class YogaDataset(Dataset):\n",
        "  def __init__(self, img_dir):\n",
        "    self.img_dir = img_dir\n",
        "    self.img_ids = os.listdir(img_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_id = self.img_ids[idx]\n",
        "    img = read_image(self.img_dir + img_id, mode=ImageReadMode.RGB)\n",
        "    img = F.resize(img, size=384, antialias=True,\n",
        "                   interpolation=InterpolationMode.BICUBIC)\n",
        "    center_crop = F.center_crop(img, output_size=384)\n",
        "    return {'img_id': img_id, 'img': img, 'center crop' : center_crop}\n",
        "\n",
        "def collate(element):\n",
        "  img = element['img']\n",
        "  img_hf = F.convert_image_dtype(F.hflip(img), dtype=torch.float32)\n",
        "  img = F.convert_image_dtype(img, dtype=torch.float32)\n",
        "  batched_imgs = torch.stack((img, img_hf), dim=0)\n",
        "\n",
        "  img = element['center crop']\n",
        "  img_hf = F.convert_image_dtype(F.hflip(img), dtype=torch.float32)\n",
        "  img = F.convert_image_dtype(img, dtype=torch.float32)\n",
        "  batched_cc = torch.stack((img, img_hf), dim=0)\n",
        "  return {'img_id' : element['img_id'],\n",
        "          'imgs' : batched_imgs, 'center crops' : batched_cc}\n",
        "\n",
        "test_data = YogaDataset(test_dir)\n",
        "test_loader = DataLoader(test_data,\n",
        "                         collate_fn=collate,\n",
        "                         shuffle=True,\n",
        "                         batch_size=None,\n",
        "                         num_workers=2)"
      ],
      "metadata": {
        "id": "lGq31rQZEudC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this to generate 1 of my 2 submissions...\n",
        "models = [model1, model2, model3]"
      ],
      "metadata": {
        "id": "_cgMGHo2OGuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ...and  this to generate the other one\n",
        "models = [model1, model2, model3, model4]"
      ],
      "metadata": {
        "id": "kmZx4W-vOP2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "img_ids = []\n",
        "labels = []\n",
        "\n",
        "for batch in tqdm(test_loader):\n",
        "  img_ids.append(batch['img_id'])\n",
        "  with torch.no_grad():\n",
        "    imgs = batch['imgs'].to(device)\n",
        "    for i, model in enumerate(models):\n",
        "      logits = model(imgs).logits\n",
        "      if i == 0:\n",
        "        predictions = logits.softmax(dim=-1)\n",
        "      else:\n",
        "        prediction = logits.softmax(dim=-1)\n",
        "        predictions = torch.cat((predictions, prediction), dim=0)\n",
        "\n",
        "    img_size = F.get_spatial_size(batch['imgs'][0])\n",
        "    if img_size[0] != img_size[1]:\n",
        "      cc_imgs = batch['center crops'].to(device)\n",
        "      for model in models:\n",
        "        logits = model(cc_imgs).logits\n",
        "        prediction = logits.softmax(dim=-1)\n",
        "        predictions = torch.cat((predictions, prediction), dim=0)\n",
        "\n",
        "    #aggregation\n",
        "    final_prediction = torch.mean(predictions, dim=0)\n",
        "    final_prediction = final_prediction.argmax().item()\n",
        "    labels.append(final_prediction)"
      ],
      "metadata": {
        "id": "P6Lw04RLGa0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "columns = {'image_id' : img_ids, 'class_6' : labels}\n",
        "\n",
        "submission = pd.DataFrame(columns)\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "submission"
      ],
      "metadata": {
        "id": "WdfY6KCKHOdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}